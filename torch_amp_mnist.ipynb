{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import torch \n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from torchmetrics.classification import F1Score, Accuracy\n",
    "from torch.optim import Optimizer, SGD\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as k\n",
    "\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 11.6\n"
     ]
    }
   ],
   "source": [
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AMP Module and Layer Configuration for Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an experimental Torch optimizer downloaded from here: https://github.com/hiyouga/AMP-Regularizer\n",
    "class AMP(Optimizer):\n",
    "    \"\"\"\n",
    "    Implements adversarial model perturbation.\n",
    "\n",
    "    Args:\n",
    "        params (iterable): iterable of trainable parameters\n",
    "        lr (float): learning rate for outer optimization\n",
    "        epsilon (float): perturbation norm ball radius\n",
    "        inner_lr (float, optional): learning rate for inner optimization (default: 1)\n",
    "        inner_iter (int, optional): iteration number for inner optimization (default: 1)\n",
    "        base_optimizer (class, optional): basic optimizer class (default: SGD)\n",
    "        **kwargs: keyword arguments passed to the `__init__` method of `base_optimizer`\n",
    "\n",
    "    Example:\n",
    "        >>> optimizer = AMP(model.parameters(), lr=0.1, eps=0.5, momentum=0.9)\n",
    "        >>> for inputs, targets in dataset:\n",
    "        >>>     def closure():\n",
    "        >>>         optimizer.zero_grad()\n",
    "        >>>         outputs = model(inputs)\n",
    "        >>>         loss = loss_fn(outputs, targets)\n",
    "        >>>         loss.backward()\n",
    "        >>>         return outputs, loss\n",
    "        >>>     outputs, loss = optimizer.step(closure)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr, epsilon, inner_lr=1, inner_iter=1, base_optimizer=SGD, **kwargs):\n",
    "        if epsilon < 0.0:\n",
    "            raise ValueError(f\"Invalid epsilon: {epsilon}\")\n",
    "        if inner_lr < 0.0:\n",
    "            raise ValueError(f\"Invalid inner lr: {inner_lr}\")\n",
    "        if inner_iter < 0:\n",
    "            raise ValueError(f\"Invalid inner iter: {inner_iter}\")\n",
    "        defaults = dict(lr=lr, epsilon=epsilon, inner_lr=inner_lr, inner_iter=inner_iter, **kwargs)\n",
    "        super(AMP, self).__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, lr=lr, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        if closure is None:\n",
    "            raise ValueError('Adversarial model perturbation requires closure, but it was not provided')\n",
    "        closure = torch.enable_grad()(closure)\n",
    "        outputs, loss = map(lambda x: x.detach(), closure())\n",
    "        for i in range(self.defaults['inner_iter']):\n",
    "            for group in self.param_groups:\n",
    "                for p in group['params']:\n",
    "                    if p.grad is not None:\n",
    "                        if i == 0:\n",
    "                            self.state[p]['dev'] = torch.zeros_like(p.grad)\n",
    "                        dev = self.state[p]['dev'] + group['inner_lr'] * p.grad\n",
    "                        clip_coef = group['epsilon'] / (dev.norm() + 1e-12)\n",
    "                        dev = clip_coef * dev if clip_coef < 1 else dev\n",
    "                        p.sub_(self.state[p]['dev']).add_(dev) # update \"theta\" with \"theta+delta\"\n",
    "                        self.state[p]['dev'] = dev\n",
    "            closure()\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is not None:\n",
    "                    p.sub_(self.state[p]['dev']) # restore \"theta\" from \"theta+delta\"\n",
    "        self.base_optimizer.step()\n",
    "        return outputs, loss\n",
    "\n",
    "# Function to instantiate the CNN.\n",
    "class CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.layers = nn.Sequential(\n",
    "      nn.Conv2d(1, 32, kernel_size=5, padding = 2),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.Dropout(0.25),\n",
    "      nn.Conv2d(32, 64, kernel_size=5, padding = 2),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.Dropout(0.25),\n",
    "      nn.Conv2d(64, 128, kernel_size=3, padding = 2),\n",
    "      nn.ReLU(),\n",
    "      nn.MaxPool2d(2),\n",
    "      nn.Dropout(0.25),\n",
    "      nn.Flatten(),\n",
    "      nn.Linear(2048,256),\n",
    "      nn.ReLU(),\n",
    "      nn.Linear(256, 64),\n",
    "      nn.ReLU(),    \n",
    "      nn.Linear(64, 10),\n",
    "      nn.Softmax(dim=1),      \n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "# Helper function to convert numpy arrays (input) to Torch tensors.\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32)).to(device)\n",
    "        self.y = torch.from_numpy(y.astype(np.int64)).to(device)\n",
    "        self.len = self.X.shape[0]\n",
    "       \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "   \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Train and Test CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calls the Torch CNN to run predictions on the provided data.\n",
    "def CNN_fit_predict(x_train, y_train, x_test, y_test):\n",
    "\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.20, random_state=42, stratify=y_train)\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # Data Loading\n",
    "    train_data = Data(X_train, np.array(Y_train))\n",
    "    val_data= Data(X_val, np.array(Y_val))\n",
    "    test_data = Data(x_test, np.array(y_test))\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=1024, shuffle=True)\n",
    "    valid_loader = DataLoader(dataset=val_data, batch_size=1024, shuffle=False)\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1024, shuffle=False)\n",
    "\n",
    "    # Instantiate CNN and configure it to use CUDA\n",
    "    cnn = CNN()\n",
    "    cnn = cnn.to(device)\n",
    "\n",
    "     # Set up loss function, optimizer, and adaptive learning rate\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = AMP(cnn.parameters(), lr=0.1, epsilon=0.75, momentum=0.95, nesterov=True, inner_lr=1,base_optimizer=torch.optim.SGD)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, factor=0.5,mode='min', min_lr=0.000001, patience=5, threshold=0.00000001)\n",
    "  \n",
    "    num_epochs = 100\n",
    "    best_val = 10000000\n",
    "    best_model_wts = None\n",
    "\n",
    "    # Run the CNN\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        train_loss = 0.0\n",
    "        y_predT = []\n",
    "        y_trn = []\n",
    "        cnn.train()\n",
    "        for X, y in train_loader:\n",
    "            # AMP needs a closure to run properly\n",
    "            def closure():\n",
    "                optimizer.zero_grad()\n",
    "                outputs = cnn(X)\n",
    "                loss = loss_function(outputs, y)\n",
    "                loss.backward()\n",
    "\n",
    "                return outputs, loss\n",
    "\n",
    "\n",
    "            outputs, loss = optimizer.step(closure)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            predicted = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "            y_predT.append(predicted)\n",
    "            y_trn.append(y.cpu())\n",
    "\n",
    "        cnn.eval()\n",
    "        valid_loss = 0.0\n",
    "        y_predV = []\n",
    "        y_vl = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # AMP needs a closure to run properly\n",
    "            for X, y in valid_loader:\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = cnn(X)\n",
    "                    loss = loss_function(outputs, y)\n",
    "                    loss.backward()\n",
    "\n",
    "                    return outputs, loss\n",
    "\n",
    "                outputs, loss = optimizer.step(closure)\n",
    "                valid_loss += loss.item()\n",
    "                predicted = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "                y_predV.append(predicted)\n",
    "                y_vl.append(y.cpu())\n",
    "\n",
    "        # Get epoch time\n",
    "        end = time.time()\n",
    "        total_time = end - start\n",
    "\n",
    "        # calculate losses/accuracy and print scores. Also save best weights based on validation loss.\n",
    "        total_training_loss = train_loss / len(train_loader)\n",
    "        total_validation_loss = valid_loss / len(valid_loader)\n",
    "\n",
    "        y_predT = list(itertools.chain(*y_predT))\n",
    "        y_trn = list(itertools.chain(*y_trn))\n",
    "\n",
    "        y_predV = list(itertools.chain(*y_predV))\n",
    "        y_vl = list(itertools.chain(*y_vl))\n",
    "\n",
    "        tf1 = accuracy_score(y_trn, y_predT)\n",
    "        vf1 = accuracy_score(y_vl, y_predV)\n",
    "\n",
    "        if total_validation_loss < best_val:\n",
    "            best_val = total_validation_loss\n",
    "            best_model_wts = copy.deepcopy(cnn.state_dict())\n",
    "\n",
    "        print(f\"Epoch {epoch+1} ({round(total_time, 2):.2f}s) => Train Loss: {round(total_training_loss,5):.5f} \\t Train Accuracy: {round(tf1,5):.5f} \\t Validation Loss: {round(total_validation_loss,5):.5f} \\t Validation Accuracy: {round(vf1,5):.5f} \\t Learning Rate: {optimizer.param_groups[0]['lr']}\")\n",
    "        scheduler.step(total_validation_loss)\n",
    "\n",
    "    # Model saves the best weights, as determined by the lowest validation loss.\n",
    "    cnn.load_state_dict(best_model_wts)\n",
    "\n",
    "    # Predict on test set\n",
    "\n",
    "    cnn.eval()\n",
    "    \n",
    "    y_predTest = []\n",
    "    y_test = []\n",
    "    with torch.no_grad():\n",
    "        for X, y in test_loader:\n",
    "            outputs = cnn(X)\n",
    "            predicted = torch.argmax(outputs, dim=1).cpu().detach().numpy()\n",
    "            y_predTest.append(predicted)\n",
    "            y_test.append(y.cpu())\n",
    "\n",
    "    y_predTest = list(itertools.chain(*y_predTest))\n",
    "    y_test = list(itertools.chain(*y_test))\n",
    "\n",
    "    print(f\"Test Accuracy: {accuracy_score(y_test, y_predTest)}\")\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Preprocess MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST from keras.\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\"\"\"\n",
    "Code for converting dataset to machine-readable format adapted from here: \n",
    "https://www.geeksforgeeks.org/applying-convolutional-neural-network-on-mnist-dataset/\n",
    "\"\"\"\n",
    "\n",
    "# Reshape arrays and convert to float.\n",
    "img_rows, img_cols=28, 28\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "# Convert to grayscale.\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "for i in range(12):\n",
    "  plt.subplot(3,4,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(x_train[i][0], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Label: {}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CNN and get Testing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (7.55s) => Train Loss: 2.30233 \t Train Accuracy: 0.10681 \t Validation Loss: 2.30183 \t Validation Accuracy: 0.11233 \t Learning Rate: 0.1\n",
      "Epoch 2 (7.26s) => Train Loss: 2.30075 \t Train Accuracy: 0.11904 \t Validation Loss: 2.29692 \t Validation Accuracy: 0.18542 \t Learning Rate: 0.1\n",
      "Epoch 3 (7.44s) => Train Loss: 2.22338 \t Train Accuracy: 0.26050 \t Validation Loss: 2.07036 \t Validation Accuracy: 0.43175 \t Learning Rate: 0.1\n",
      "Epoch 4 (7.38s) => Train Loss: 1.90966 \t Train Accuracy: 0.55092 \t Validation Loss: 1.80466 \t Validation Accuracy: 0.65183 \t Learning Rate: 0.1\n",
      "Epoch 5 (7.21s) => Train Loss: 1.77888 \t Train Accuracy: 0.67719 \t Validation Loss: 1.71321 \t Validation Accuracy: 0.75092 \t Learning Rate: 0.1\n",
      "Epoch 6 (7.26s) => Train Loss: 1.63374 \t Train Accuracy: 0.83144 \t Validation Loss: 1.59113 \t Validation Accuracy: 0.86842 \t Learning Rate: 0.1\n",
      "Epoch 7 (7.44s) => Train Loss: 1.59161 \t Train Accuracy: 0.86883 \t Validation Loss: 1.56480 \t Validation Accuracy: 0.90100 \t Learning Rate: 0.1\n",
      "Epoch 8 (7.40s) => Train Loss: 1.52316 \t Train Accuracy: 0.93944 \t Validation Loss: 1.49499 \t Validation Accuracy: 0.96650 \t Learning Rate: 0.1\n",
      "Epoch 9 (7.20s) => Train Loss: 1.49896 \t Train Accuracy: 0.96240 \t Validation Loss: 1.48519 \t Validation Accuracy: 0.97667 \t Learning Rate: 0.1\n",
      "Epoch 10 (7.25s) => Train Loss: 1.49076 \t Train Accuracy: 0.97123 \t Validation Loss: 1.48185 \t Validation Accuracy: 0.97958 \t Learning Rate: 0.1\n",
      "Epoch 11 (7.22s) => Train Loss: 1.48709 \t Train Accuracy: 0.97448 \t Validation Loss: 1.47864 \t Validation Accuracy: 0.98267 \t Learning Rate: 0.1\n",
      "Epoch 12 (7.40s) => Train Loss: 1.48446 \t Train Accuracy: 0.97735 \t Validation Loss: 1.47720 \t Validation Accuracy: 0.98408 \t Learning Rate: 0.1\n",
      "Epoch 13 (7.37s) => Train Loss: 1.48244 \t Train Accuracy: 0.97935 \t Validation Loss: 1.47595 \t Validation Accuracy: 0.98483 \t Learning Rate: 0.1\n",
      "Epoch 14 (7.18s) => Train Loss: 1.47974 \t Train Accuracy: 0.98188 \t Validation Loss: 1.47367 \t Validation Accuracy: 0.98825 \t Learning Rate: 0.1\n",
      "Epoch 15 (7.22s) => Train Loss: 1.47872 \t Train Accuracy: 0.98294 \t Validation Loss: 1.47335 \t Validation Accuracy: 0.98800 \t Learning Rate: 0.1\n",
      "Epoch 16 (7.36s) => Train Loss: 1.47747 \t Train Accuracy: 0.98375 \t Validation Loss: 1.47211 \t Validation Accuracy: 0.98958 \t Learning Rate: 0.1\n",
      "Epoch 17 (7.25s) => Train Loss: 1.47640 \t Train Accuracy: 0.98502 \t Validation Loss: 1.47088 \t Validation Accuracy: 0.99092 \t Learning Rate: 0.1\n",
      "Epoch 18 (7.10s) => Train Loss: 1.47559 \t Train Accuracy: 0.98577 \t Validation Loss: 1.46946 \t Validation Accuracy: 0.99192 \t Learning Rate: 0.1\n",
      "Epoch 19 (7.10s) => Train Loss: 1.47517 \t Train Accuracy: 0.98629 \t Validation Loss: 1.46967 \t Validation Accuracy: 0.99183 \t Learning Rate: 0.1\n",
      "Epoch 20 (7.24s) => Train Loss: 1.47407 \t Train Accuracy: 0.98748 \t Validation Loss: 1.46897 \t Validation Accuracy: 0.99258 \t Learning Rate: 0.1\n",
      "Epoch 21 (7.10s) => Train Loss: 1.47366 \t Train Accuracy: 0.98765 \t Validation Loss: 1.46907 \t Validation Accuracy: 0.99242 \t Learning Rate: 0.1\n",
      "Epoch 22 (7.10s) => Train Loss: 1.47310 \t Train Accuracy: 0.98827 \t Validation Loss: 1.46868 \t Validation Accuracy: 0.99292 \t Learning Rate: 0.1\n",
      "Epoch 23 (7.11s) => Train Loss: 1.47297 \t Train Accuracy: 0.98862 \t Validation Loss: 1.46780 \t Validation Accuracy: 0.99383 \t Learning Rate: 0.1\n",
      "Epoch 24 (7.30s) => Train Loss: 1.47232 \t Train Accuracy: 0.98925 \t Validation Loss: 1.46822 \t Validation Accuracy: 0.99325 \t Learning Rate: 0.1\n",
      "Epoch 25 (7.18s) => Train Loss: 1.47234 \t Train Accuracy: 0.98915 \t Validation Loss: 1.46792 \t Validation Accuracy: 0.99333 \t Learning Rate: 0.1\n",
      "Epoch 26 (7.21s) => Train Loss: 1.47074 \t Train Accuracy: 0.99079 \t Validation Loss: 1.46742 \t Validation Accuracy: 0.99408 \t Learning Rate: 0.1\n",
      "Epoch 27 (7.21s) => Train Loss: 1.47068 \t Train Accuracy: 0.99079 \t Validation Loss: 1.46708 \t Validation Accuracy: 0.99450 \t Learning Rate: 0.1\n",
      "Epoch 28 (7.39s) => Train Loss: 1.47098 \t Train Accuracy: 0.99060 \t Validation Loss: 1.46753 \t Validation Accuracy: 0.99383 \t Learning Rate: 0.1\n",
      "Epoch 29 (7.21s) => Train Loss: 1.47062 \t Train Accuracy: 0.99075 \t Validation Loss: 1.46700 \t Validation Accuracy: 0.99433 \t Learning Rate: 0.1\n",
      "Epoch 30 (7.22s) => Train Loss: 1.46989 \t Train Accuracy: 0.99165 \t Validation Loss: 1.46694 \t Validation Accuracy: 0.99433 \t Learning Rate: 0.1\n",
      "Epoch 31 (7.15s) => Train Loss: 1.46985 \t Train Accuracy: 0.99150 \t Validation Loss: 1.46668 \t Validation Accuracy: 0.99492 \t Learning Rate: 0.1\n",
      "Epoch 32 (7.32s) => Train Loss: 1.47002 \t Train Accuracy: 0.99125 \t Validation Loss: 1.46699 \t Validation Accuracy: 0.99433 \t Learning Rate: 0.1\n",
      "Epoch 33 (7.21s) => Train Loss: 1.46961 \t Train Accuracy: 0.99190 \t Validation Loss: 1.46624 \t Validation Accuracy: 0.99500 \t Learning Rate: 0.1\n",
      "Epoch 34 (7.05s) => Train Loss: 1.46937 \t Train Accuracy: 0.99198 \t Validation Loss: 1.46694 \t Validation Accuracy: 0.99433 \t Learning Rate: 0.1\n",
      "Epoch 35 (7.05s) => Train Loss: 1.46952 \t Train Accuracy: 0.99181 \t Validation Loss: 1.46593 \t Validation Accuracy: 0.99542 \t Learning Rate: 0.1\n",
      "Epoch 36 (7.03s) => Train Loss: 1.46892 \t Train Accuracy: 0.99250 \t Validation Loss: 1.46642 \t Validation Accuracy: 0.99483 \t Learning Rate: 0.1\n",
      "Epoch 37 (7.20s) => Train Loss: 1.46852 \t Train Accuracy: 0.99288 \t Validation Loss: 1.46666 \t Validation Accuracy: 0.99492 \t Learning Rate: 0.1\n",
      "Epoch 38 (7.04s) => Train Loss: 1.46857 \t Train Accuracy: 0.99292 \t Validation Loss: 1.46652 \t Validation Accuracy: 0.99475 \t Learning Rate: 0.1\n",
      "Epoch 39 (7.03s) => Train Loss: 1.46832 \t Train Accuracy: 0.99302 \t Validation Loss: 1.46623 \t Validation Accuracy: 0.99500 \t Learning Rate: 0.1\n",
      "Epoch 40 (7.25s) => Train Loss: 1.46808 \t Train Accuracy: 0.99323 \t Validation Loss: 1.46615 \t Validation Accuracy: 0.99525 \t Learning Rate: 0.1\n",
      "Epoch 41 (7.33s) => Train Loss: 1.46878 \t Train Accuracy: 0.99250 \t Validation Loss: 1.46698 \t Validation Accuracy: 0.99417 \t Learning Rate: 0.1\n",
      "Epoch 42 (7.14s) => Train Loss: 1.46827 \t Train Accuracy: 0.99310 \t Validation Loss: 1.46561 \t Validation Accuracy: 0.99575 \t Learning Rate: 0.05\n",
      "Epoch 43 (7.13s) => Train Loss: 1.46747 \t Train Accuracy: 0.99381 \t Validation Loss: 1.46571 \t Validation Accuracy: 0.99558 \t Learning Rate: 0.05\n",
      "Epoch 44 (7.16s) => Train Loss: 1.46747 \t Train Accuracy: 0.99392 \t Validation Loss: 1.46560 \t Validation Accuracy: 0.99558 \t Learning Rate: 0.05\n",
      "Epoch 45 (7.38s) => Train Loss: 1.46736 \t Train Accuracy: 0.99398 \t Validation Loss: 1.46561 \t Validation Accuracy: 0.99550 \t Learning Rate: 0.05\n",
      "Epoch 46 (7.15s) => Train Loss: 1.46710 \t Train Accuracy: 0.99415 \t Validation Loss: 1.46544 \t Validation Accuracy: 0.99583 \t Learning Rate: 0.05\n",
      "Epoch 47 (7.14s) => Train Loss: 1.46700 \t Train Accuracy: 0.99427 \t Validation Loss: 1.46540 \t Validation Accuracy: 0.99575 \t Learning Rate: 0.05\n",
      "Epoch 48 (7.16s) => Train Loss: 1.46711 \t Train Accuracy: 0.99408 \t Validation Loss: 1.46577 \t Validation Accuracy: 0.99558 \t Learning Rate: 0.05\n",
      "Epoch 49 (7.30s) => Train Loss: 1.46733 \t Train Accuracy: 0.99417 \t Validation Loss: 1.46570 \t Validation Accuracy: 0.99567 \t Learning Rate: 0.05\n",
      "Epoch 50 (7.15s) => Train Loss: 1.46631 \t Train Accuracy: 0.99504 \t Validation Loss: 1.46540 \t Validation Accuracy: 0.99575 \t Learning Rate: 0.05\n",
      "Epoch 51 (7.34s) => Train Loss: 1.46685 \t Train Accuracy: 0.99454 \t Validation Loss: 1.46524 \t Validation Accuracy: 0.99583 \t Learning Rate: 0.05\n",
      "Epoch 52 (7.38s) => Train Loss: 1.46651 \t Train Accuracy: 0.99475 \t Validation Loss: 1.46535 \t Validation Accuracy: 0.99592 \t Learning Rate: 0.05\n",
      "Epoch 53 (7.22s) => Train Loss: 1.46657 \t Train Accuracy: 0.99467 \t Validation Loss: 1.46520 \t Validation Accuracy: 0.99600 \t Learning Rate: 0.05\n",
      "Epoch 54 (7.10s) => Train Loss: 1.46627 \t Train Accuracy: 0.99508 \t Validation Loss: 1.46529 \t Validation Accuracy: 0.99600 \t Learning Rate: 0.05\n",
      "Epoch 55 (6.93s) => Train Loss: 1.46653 \t Train Accuracy: 0.99483 \t Validation Loss: 1.46506 \t Validation Accuracy: 0.99625 \t Learning Rate: 0.05\n",
      "Epoch 56 (6.95s) => Train Loss: 1.46650 \t Train Accuracy: 0.99479 \t Validation Loss: 1.46528 \t Validation Accuracy: 0.99592 \t Learning Rate: 0.05\n",
      "Epoch 57 (6.96s) => Train Loss: 1.46656 \t Train Accuracy: 0.99477 \t Validation Loss: 1.46526 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.05\n",
      "Epoch 58 (7.12s) => Train Loss: 1.46607 \t Train Accuracy: 0.99546 \t Validation Loss: 1.46519 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.05\n",
      "Epoch 59 (6.98s) => Train Loss: 1.46588 \t Train Accuracy: 0.99540 \t Validation Loss: 1.46521 \t Validation Accuracy: 0.99600 \t Learning Rate: 0.05\n",
      "Epoch 60 (7.04s) => Train Loss: 1.46590 \t Train Accuracy: 0.99548 \t Validation Loss: 1.46541 \t Validation Accuracy: 0.99583 \t Learning Rate: 0.05\n",
      "Epoch 61 (7.22s) => Train Loss: 1.46626 \t Train Accuracy: 0.99508 \t Validation Loss: 1.46535 \t Validation Accuracy: 0.99583 \t Learning Rate: 0.05\n",
      "Epoch 62 (7.22s) => Train Loss: 1.46616 \t Train Accuracy: 0.99529 \t Validation Loss: 1.46515 \t Validation Accuracy: 0.99600 \t Learning Rate: 0.025\n",
      "Epoch 63 (7.04s) => Train Loss: 1.46608 \t Train Accuracy: 0.99521 \t Validation Loss: 1.46509 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.025\n",
      "Epoch 64 (7.03s) => Train Loss: 1.46532 \t Train Accuracy: 0.99606 \t Validation Loss: 1.46524 \t Validation Accuracy: 0.99592 \t Learning Rate: 0.025\n",
      "Epoch 65 (7.03s) => Train Loss: 1.46552 \t Train Accuracy: 0.99579 \t Validation Loss: 1.46520 \t Validation Accuracy: 0.99592 \t Learning Rate: 0.025\n",
      "Epoch 66 (7.21s) => Train Loss: 1.46564 \t Train Accuracy: 0.99567 \t Validation Loss: 1.46516 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.025\n",
      "Epoch 67 (7.03s) => Train Loss: 1.46554 \t Train Accuracy: 0.99567 \t Validation Loss: 1.46511 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.025\n",
      "Epoch 68 (6.93s) => Train Loss: 1.46521 \t Train Accuracy: 0.99606 \t Validation Loss: 1.46510 \t Validation Accuracy: 0.99617 \t Learning Rate: 0.0125\n",
      "Epoch 69 (6.94s) => Train Loss: 1.46535 \t Train Accuracy: 0.99590 \t Validation Loss: 1.46511 \t Validation Accuracy: 0.99600 \t Learning Rate: 0.0125\n",
      "Epoch 70 (7.57s) => Train Loss: 1.46540 \t Train Accuracy: 0.99604 \t Validation Loss: 1.46505 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 71 (7.57s) => Train Loss: 1.46531 \t Train Accuracy: 0.99606 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 72 (7.31s) => Train Loss: 1.46530 \t Train Accuracy: 0.99598 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 73 (7.11s) => Train Loss: 1.46537 \t Train Accuracy: 0.99588 \t Validation Loss: 1.46501 \t Validation Accuracy: 0.99617 \t Learning Rate: 0.0125\n",
      "Epoch 74 (7.72s) => Train Loss: 1.46528 \t Train Accuracy: 0.99596 \t Validation Loss: 1.46507 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 75 (7.66s) => Train Loss: 1.46538 \t Train Accuracy: 0.99588 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 76 (7.14s) => Train Loss: 1.46527 \t Train Accuracy: 0.99608 \t Validation Loss: 1.46499 \t Validation Accuracy: 0.99617 \t Learning Rate: 0.0125\n",
      "Epoch 77 (7.03s) => Train Loss: 1.46542 \t Train Accuracy: 0.99602 \t Validation Loss: 1.46508 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 78 (7.03s) => Train Loss: 1.46493 \t Train Accuracy: 0.99652 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 79 (7.10s) => Train Loss: 1.46529 \t Train Accuracy: 0.99600 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 80 (6.92s) => Train Loss: 1.46552 \t Train Accuracy: 0.99567 \t Validation Loss: 1.46505 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 81 (6.91s) => Train Loss: 1.46502 \t Train Accuracy: 0.99633 \t Validation Loss: 1.46501 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 82 (7.09s) => Train Loss: 1.46535 \t Train Accuracy: 0.99600 \t Validation Loss: 1.46505 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0125\n",
      "Epoch 83 (7.09s) => Train Loss: 1.46488 \t Train Accuracy: 0.99652 \t Validation Loss: 1.46506 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 84 (6.95s) => Train Loss: 1.46506 \t Train Accuracy: 0.99615 \t Validation Loss: 1.46505 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 85 (6.94s) => Train Loss: 1.46500 \t Train Accuracy: 0.99642 \t Validation Loss: 1.46501 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 86 (6.94s) => Train Loss: 1.46522 \t Train Accuracy: 0.99608 \t Validation Loss: 1.46502 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 87 (7.09s) => Train Loss: 1.46513 \t Train Accuracy: 0.99619 \t Validation Loss: 1.46501 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 88 (6.97s) => Train Loss: 1.46527 \t Train Accuracy: 0.99608 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.00625\n",
      "Epoch 89 (7.56s) => Train Loss: 1.46481 \t Train Accuracy: 0.99654 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 90 (7.45s) => Train Loss: 1.46496 \t Train Accuracy: 0.99638 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 91 (7.18s) => Train Loss: 1.46494 \t Train Accuracy: 0.99635 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 92 (6.92s) => Train Loss: 1.46502 \t Train Accuracy: 0.99629 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 93 (7.35s) => Train Loss: 1.46526 \t Train Accuracy: 0.99612 \t Validation Loss: 1.46502 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 94 (7.44s) => Train Loss: 1.46496 \t Train Accuracy: 0.99644 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.003125\n",
      "Epoch 95 (7.67s) => Train Loss: 1.46491 \t Train Accuracy: 0.99640 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Epoch 96 (7.69s) => Train Loss: 1.46482 \t Train Accuracy: 0.99642 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Epoch 97 (7.57s) => Train Loss: 1.46462 \t Train Accuracy: 0.99685 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Epoch 98 (7.55s) => Train Loss: 1.46500 \t Train Accuracy: 0.99633 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Epoch 99 (7.52s) => Train Loss: 1.46528 \t Train Accuracy: 0.99608 \t Validation Loss: 1.46504 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Epoch 100 (7.73s) => Train Loss: 1.46525 \t Train Accuracy: 0.99612 \t Validation Loss: 1.46503 \t Validation Accuracy: 0.99608 \t Learning Rate: 0.0015625\n",
      "Test Accuracy: 0.9951\n"
     ]
    }
   ],
   "source": [
    "CNN_fit_predict(x_train, y_train, x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4593b52a31704b5155580280f0f05a500adb36bf97584c7e94e26dfe17ea16d9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
